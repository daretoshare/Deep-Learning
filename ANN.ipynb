{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ANN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daretoshare/Deep-Learning/blob/master/ANN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igh0aRySK-qW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ff0b716c-45e5-43af-b32a-b2bf2a07f75a"
      },
      "source": [
        "# Importing the libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GATuNQ5gMFiy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Importing the dataset\n",
        "dataset = pd.read_csv('Churn_Modelling.csv')\n",
        "X = dataset.iloc[:, 3:13].values\n",
        "y = dataset.iloc[:, 13].values\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qF29vkYBPfE4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "73e98674-2365-4ee0-a8e4-d7b8b0d56681"
      },
      "source": [
        "X"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[619, 'France', 'Female', ..., 1, 1, 101348.88],\n",
              "       [608, 'Spain', 'Female', ..., 0, 1, 112542.58],\n",
              "       [502, 'France', 'Female', ..., 1, 0, 113931.57],\n",
              "       ...,\n",
              "       [709, 'France', 'Female', ..., 0, 1, 42085.58],\n",
              "       [772, 'Germany', 'Male', ..., 1, 0, 92888.52],\n",
              "       [792, 'France', 'Female', ..., 1, 0, 38190.78]], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIeyizQfMgeo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "604a5b68-7502-4335-8c30-07bbb092e54f"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "labelencoder_X_country = LabelEncoder()\n",
        "X[:, 1] = labelencoder_X_country.fit_transform(X[:, 1])\n",
        "labelencoder_X_gender = LabelEncoder()\n",
        "X[:, 2] = labelencoder_X_gender.fit_transform(X[:, 2])\n",
        "onehotencoder_country = OneHotEncoder(categorical_features = [1])\n",
        "X = onehotencoder_country.fit_transform(X).toarray()\n",
        "X = X[:, 1:]\n",
        "\n",
        "# Splitting the dataset into the Training set and Test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
        "\n",
        "# Feature Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "\n",
        "\n",
        "def build_classifier(optimizer):\n",
        "        classifier = Sequential()\n",
        "        classifier.add(Dense(units = 6, activation= 'relu', input_dim = 11 ))\n",
        "        classifier.add(Dense(units = 6, activation= 'relu'))\n",
        "        classifier.add(Dense(units = 6, activation= 'relu'))\n",
        "        classifier.add(Dense(units = 1, activation= 'sigmoid'))\n",
        "        classifier.compile(optimizer=optimizer, loss ='binary_crossentropy', metrics =['accuracy'])\n",
        "        return classifier\n",
        "       \n",
        "classifier = KerasClassifier(build_fn = build_classifier)\n",
        "parameters = {\n",
        "        \n",
        "        'batch_size' : [30, 60],\n",
        "              'nb_epoch' : [10, 15],\n",
        "              'optimizer' : ['adam' , 'rmsprop']\n",
        "               }\n",
        "grid_search = GridSearchCV(estimator = classifier,\n",
        "                           param_grid = parameters,\n",
        "                           scoring = 'accuracy',\n",
        "                           cv = 10 ,\n",
        "                           n_jobs =-1)\n",
        "grid_search.fit(X_train, y_train)      \n",
        "\n",
        "best_param = grid_search.best_params_\n",
        "best_accuracy = grid_search.best_score_\n",
        "\n",
        "print(best_param)\n",
        "print(best_accuracy)\n",
        "       "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
            "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
            "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:451: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
            "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 5s 604us/step - loss: 0.5386 - acc: 0.7957\n",
            "{'batch_size': 30, 'nb_epoch': 10, 'optimizer': 'rmsprop'}\n",
            "0.796\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0LLq5tQUJO6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 843
        },
        "outputId": "3572fc4e-65a5-4664-d6f2-d0e79b6bdf42"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Sat Jun  8 22:16:20 2019\n",
        "\n",
        "@author: Dipanjan\n",
        "\"\"\"\n",
        "# Importing the libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Importing the dataset\n",
        "dataset = pd.read_csv('Churn_Modelling.csv')\n",
        "X = dataset.iloc[:, 3:13].values\n",
        "y = dataset.iloc[:, 13].values\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "labelencoder_X_country = LabelEncoder()\n",
        "X[:, 1] = labelencoder_X_country.fit_transform(X[:, 1])\n",
        "labelencoder_X_gender = LabelEncoder()\n",
        "X[:, 2] = labelencoder_X_gender.fit_transform(X[:, 2])\n",
        "onehotencoder_country = OneHotEncoder(categorical_features = [1])\n",
        "X = onehotencoder_country.fit_transform(X).toarray()\n",
        "X = X[:, 1:]\n",
        "\n",
        "# Splitting the dataset into the Training set and Test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
        "\n",
        "# Feature Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "classifier = Sequential()\n",
        "classifier.add(Dense(units = 6, activation= 'relu', input_dim = 11 ))\n",
        "classifier.add(Dense(units = 6, activation= 'relu'))\n",
        "classifier.add(Dense(units = 6, activation= 'relu'))\n",
        "classifier.add(Dense(units = 1, activation= 'sigmoid'))\n",
        "classifier.compile(optimizer='adam', loss ='binary_crossentropy', metrics =['accuracy'])\n",
        "\n",
        "classifier.fit(X_train, y_train, batch_size= 50, epochs=20)\n",
        "y_pred = classifier.predict(X_test)\n",
        "y_pred = (y_pred > .5)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cm\n",
        "\n",
        "#Use our ANN model to predict if the customer with the following informations will leave the bank: \n",
        "#\n",
        "#Geography: France\n",
        "#Credit Score: 600\n",
        "#Gender: Male\n",
        "#Age: 40 years old\n",
        "#Tenure: 3 years\n",
        "#Balance: $60000\n",
        "#Number of Products: 2\n",
        "#Does this customer have a credit card ? Yes\n",
        "#Is this customer an Active Member: Yes\n",
        "#Estimated Salary: $50000\n",
        "#    \n",
        "#So should we say goodbye to that customer ?\n",
        "\n",
        "\n",
        "# Need to hand code by refering the encoders (trick is to convert the array to horizontal array by double sq brackets)\n",
        "new_record = np.array([[0.0,0,600,1,40,3,60000,2,1,1,50000]])\n",
        "new_record_scalled = sc.fit_transform(new_record)\n",
        "\n",
        "new_pred = classifier.predict(new_record_scalled)\n",
        "print(new_pred)\n",
        "print((new_pred > .5))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
            "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
            "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:451: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
            "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "8000/8000 [==============================] - 5s 583us/step - loss: 0.6250 - acc: 0.7347\n",
            "Epoch 2/20\n",
            "8000/8000 [==============================] - 0s 47us/step - loss: 0.4923 - acc: 0.7970\n",
            "Epoch 3/20\n",
            "8000/8000 [==============================] - 0s 48us/step - loss: 0.4570 - acc: 0.8007\n",
            "Epoch 4/20\n",
            "8000/8000 [==============================] - 0s 48us/step - loss: 0.4416 - acc: 0.8045\n",
            "Epoch 5/20\n",
            "8000/8000 [==============================] - 0s 47us/step - loss: 0.4330 - acc: 0.8095\n",
            "Epoch 6/20\n",
            "8000/8000 [==============================] - 0s 48us/step - loss: 0.4276 - acc: 0.8149\n",
            "Epoch 7/20\n",
            "8000/8000 [==============================] - 0s 47us/step - loss: 0.4229 - acc: 0.8171\n",
            "Epoch 8/20\n",
            "8000/8000 [==============================] - 0s 48us/step - loss: 0.4188 - acc: 0.8207\n",
            "Epoch 9/20\n",
            "8000/8000 [==============================] - 0s 46us/step - loss: 0.4138 - acc: 0.8224\n",
            "Epoch 10/20\n",
            "8000/8000 [==============================] - 0s 50us/step - loss: 0.4088 - acc: 0.8245\n",
            "Epoch 11/20\n",
            "8000/8000 [==============================] - 0s 48us/step - loss: 0.4044 - acc: 0.8272\n",
            "Epoch 12/20\n",
            "8000/8000 [==============================] - 0s 47us/step - loss: 0.3989 - acc: 0.8312\n",
            "Epoch 13/20\n",
            "8000/8000 [==============================] - 0s 46us/step - loss: 0.3940 - acc: 0.8327\n",
            "Epoch 14/20\n",
            "8000/8000 [==============================] - 0s 47us/step - loss: 0.3882 - acc: 0.8367\n",
            "Epoch 15/20\n",
            "8000/8000 [==============================] - 0s 47us/step - loss: 0.3802 - acc: 0.8404\n",
            "Epoch 16/20\n",
            "8000/8000 [==============================] - 0s 47us/step - loss: 0.3719 - acc: 0.8480\n",
            "Epoch 17/20\n",
            "8000/8000 [==============================] - 0s 47us/step - loss: 0.3629 - acc: 0.8529\n",
            "Epoch 18/20\n",
            "8000/8000 [==============================] - 0s 45us/step - loss: 0.3559 - acc: 0.8556\n",
            "Epoch 19/20\n",
            "8000/8000 [==============================] - 0s 48us/step - loss: 0.3510 - acc: 0.8566\n",
            "Epoch 20/20\n",
            "8000/8000 [==============================] - 0s 47us/step - loss: 0.3469 - acc: 0.8605\n",
            "[[0.07003798]]\n",
            "[[False]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}