{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ANN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daretoshare/Deep-Learning/blob/master/ANN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igh0aRySK-qW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ff0b716c-45e5-43af-b32a-b2bf2a07f75a"
      },
      "source": [
        "# Importing the libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GATuNQ5gMFiy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Importing the dataset\n",
        "dataset = pd.read_csv('Churn_Modelling.csv')\n",
        "X = dataset.iloc[:, 3:13].values\n",
        "y = dataset.iloc[:, 13].values\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qF29vkYBPfE4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "73e98674-2365-4ee0-a8e4-d7b8b0d56681"
      },
      "source": [
        "X"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[619, 'France', 'Female', ..., 1, 1, 101348.88],\n",
              "       [608, 'Spain', 'Female', ..., 0, 1, 112542.58],\n",
              "       [502, 'France', 'Female', ..., 1, 0, 113931.57],\n",
              "       ...,\n",
              "       [709, 'France', 'Female', ..., 0, 1, 42085.58],\n",
              "       [772, 'Germany', 'Male', ..., 1, 0, 92888.52],\n",
              "       [792, 'France', 'Female', ..., 1, 0, 38190.78]], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIeyizQfMgeo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "604a5b68-7502-4335-8c30-07bbb092e54f"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "labelencoder_X_country = LabelEncoder()\n",
        "X[:, 1] = labelencoder_X_country.fit_transform(X[:, 1])\n",
        "labelencoder_X_gender = LabelEncoder()\n",
        "X[:, 2] = labelencoder_X_gender.fit_transform(X[:, 2])\n",
        "onehotencoder_country = OneHotEncoder(categorical_features = [1])\n",
        "X = onehotencoder_country.fit_transform(X).toarray()\n",
        "X = X[:, 1:]\n",
        "\n",
        "# Splitting the dataset into the Training set and Test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
        "\n",
        "# Feature Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "\n",
        "\n",
        "def build_classifier(optimizer):\n",
        "        classifier = Sequential()\n",
        "        classifier.add(Dense(units = 6, activation= 'relu', input_dim = 11 ))\n",
        "        classifier.add(Dense(units = 6, activation= 'relu'))\n",
        "        classifier.add(Dense(units = 6, activation= 'relu'))\n",
        "        classifier.add(Dense(units = 1, activation= 'sigmoid'))\n",
        "        classifier.compile(optimizer=optimizer, loss ='binary_crossentropy', metrics =['accuracy'])\n",
        "        return classifier\n",
        "       \n",
        "classifier = KerasClassifier(build_fn = build_classifier)\n",
        "parameters = {\n",
        "        \n",
        "        'batch_size' : [30, 60],\n",
        "              'nb_epoch' : [10, 15],\n",
        "              'optimizer' : ['adam' , 'rmsprop']\n",
        "               }\n",
        "grid_search = GridSearchCV(estimator = classifier,\n",
        "                           param_grid = parameters,\n",
        "                           scoring = 'accuracy',\n",
        "                           cv = 10 ,\n",
        "                           n_jobs =-1)\n",
        "grid_search.fit(X_train, y_train)      \n",
        "\n",
        "best_param = grid_search.best_params_\n",
        "best_accuracy = grid_search.best_score_\n",
        "\n",
        "print(best_param)\n",
        "print(best_accuracy)\n",
        "       "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
            "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
            "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:451: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
            "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "8000/8000 [==============================] - 5s 604us/step - loss: 0.5386 - acc: 0.7957\n",
            "{'batch_size': 30, 'nb_epoch': 10, 'optimizer': 'rmsprop'}\n",
            "0.796\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}